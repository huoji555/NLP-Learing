## 词典分词

###### 1.概述

`中文分词`指的是将一段文本拆分为一系列单词的过程，这是中文信息处理的第一站，中文分词备受关注。中文分词大致分为以下两类：

- **基于词典规则**
- **基于机器学习**

这里我们主要介绍词典分词



###### 2.词典分词

**词典分词**是最简单，最常见的分词算法，需要的材料为：

- **一部词典**
- **一套查词典的规则**

简单来说，词典分词就是一个确定的查词与输出的规则系统。词典分词的重点不在于分词本身，而在于**支撑词典的数据结构**。



###### 3.词典分类及加载

互联网上有许多公开的中文词典，比如搜狗实验室发布的互联网词库`SogouW`，清华大学开放中文词库`THUOCL`，以及千万级巨型汉语词库`HanLP`。我们这里用`HanLP`



`HanLP`中词典的格式是一种以空格分隔的表格形式，第一列是单词本身，之后表示词性与相应的词频，下面举个栗子：

```java
希望  v  386  n 96
希罕  a  1
希冀  v  1
```



下载的话，需要先下载好`HanLP`的数据包，推荐从`码云`直接[下载](<https://gitee.com/weiyy153/HanLP/tree/1.x/data>)，`GitHub`很慢。

加载的话很简单，我们把下载好的数据包放在`src`目录下，只需要以下的代码即可完成字典的加载：

```java
TreeMap<String,CoreDictionary.Attribute> dictionary = IOUtil.loadDictionary("src/HanLP/data/dictionary/CoreNatureDictionary.mini.txt");
```